[
["textmining.html", "Kapitel 11 Textmining 11.1 Einführung 11.2 Grundlegende Analyse 11.3 Sentiment-Analyse 11.4 Vertiefung", " Kapitel 11 Textmining Ein großer Teil der zur Verfügung stehenden Daten liegt nicht als braves Zahlenmaterial vor, sondern in “unstrukturierter” Form, z.B. in Form von Texten. Im Gegensatz zur Analyse von numerischen Daten ist die Analyse von Texten1 weniger verbreitet bisher. In Anbetracht der Menge und der Informationsreichhaltigkeit von Text erscheint die Analyse von Text als vielversprechend. In gewisser Weise ist das Textmining ein alternative zu klassischen qualitativen Verfahren der Sozialforschung. Geht es in der qualitativen Sozialforschung primär um das Verstehen eines Textes, so kann man für das Textmining ähnliche Ziele formulieren. Allerdings: Das Textmining ist wesentlich schwächer und beschränkter in der Tiefe des Verstehens. Der Computer ist einfach noch wesentlich dümmer als ein Mensch, in dieser Hinsicht. Allerdings ist er auch wesentlich schneller als ein Mensch, was das Lesen betrifft. Daher bietet sich das Textmining für das Lesen großer Textmengen an, in denen eine geringe Informationsdichte vermutet wird. Sozusagen maschinelles Sieben im großen Stil. Da fällt viel durch die Maschen, aber es werden Tonnen von Sand bewegt. In der Regel wird das Textmining als gemischte Methode verwendet: sowohl qualitative als auch qualitative Aspekte spielen eine Rolle. Damit vermittelt das Textmining auf konstruktive Art und Weise zwischen den manchmal antagonierenden Schulen der qualitativ-idiographischen und der quantitativ-nomothetischen Sichtweise auf die Welt. Man könnte es auch als qualitative Forschung mit moderner Technik bezeichnen - mit den skizzierten Einschränkungen wohlgemerkt. 11.1 Einführung 11.1.1 Grundbegriffe Die computergestützte Analyse von Texten speiste (und speist) sich reichhaltig aus Quellen der Linguistik; entsprechende Fachtermini finden Verwendung: Ein Corpus bezeichnet die Menge der zu analyisierenden Dokumente; das könnten z.B. alle Reden der Bundeskanzlerin Angela Merkel sein oder alle Tweets von “@realDonaldTrump”. Ein Token (Term) ist ein elementarer Baustein eines Texts, die kleinste Analyseeinheit, häufig ein Wort. Unter tidy text versteht man einen Dataframe, in dem pro Zeile nur ein Term steht (Silge and Robinson 2016). 11.1.2 Software Für dieses Kapitel benötigen Sie R, RStudio sowie diese R-Pakete: install.packages(c(&quot;tidytext&quot;, &quot;tidyverse&quot;, &quot;pdftools&quot;, &quot;downloader&quot;, &quot;ggdendro&quot;, &quot;okcupiddata&quot;, &quot;gridExtra&quot; &quot;lsa&quot;, &quot;knitr&quot;, &quot;SnowballC&quot;, &quot;wordcloud&quot;), dependencies = TRUE) 11.2 Grundlegende Analyse 11.2.1 Tidy Text Dataframes Basteln wir uns einen tidy text Dataframe. Wir gehen dabei von einem Vektor mit mehreren Text-Elementen aus, das ist ein realistischer Startpunkt. Unser Text-Vektor2 besteht aus 4 Elementen. text &lt;- c(&quot;Wir haben die Frauen zu Bett gebracht,&quot;, &quot;als die Männer in Frankreich standen.&quot;, &quot;Wir hatten uns das viel schöner gedacht.&quot;, &quot;Wir waren nur Konfirmanden.&quot;) Als nächstes machen wir daraus einen Dataframe. library(tidyverse) text_df &lt;- data_frame(Zeile = 1:4, text) Zeile text 1 Wir haben die Frauen zu Bett gebracht, 2 als die Männer in Frankreich standen. 3 Wir hatten uns das viel schöner gedacht. 4 Wir waren nur Konfirmanden. Und “dehnen” diesen Dataframe zu einem tidy text Dataframe. library(tidytext) text_df %&gt;% unnest_tokens(wort, text) #&gt; # A tibble: 24 × 2 #&gt; Zeile wort #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 wir #&gt; 2 1 haben #&gt; 3 1 die #&gt; # ... with 21 more rows Das unnest_tokens kann übersetzt werden als “entschachtele” oder “dehne” die Tokens - so dass in jeder Zeile nur noch ein Wort (Token) steht. Die Syntax ist unnest_tokens(Ausgabespalte, Eingabespalte). Nebenbei werden übrigens alle Buchstaben auf Kleinschreibung getrimmt. Als nächstes filtern wir die Satzzeichen heraus, da die Wörter für die Analyse wichtiger (oder zumindest einfacher) sind. library(stringr) text_df %&gt;% unnest_tokens(wort, text) %&gt;% filter(str_detect(wort, &quot;[a-z]&quot;)) #&gt; # A tibble: 24 × 2 #&gt; Zeile wort #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 wir #&gt; 2 1 haben #&gt; 3 1 die #&gt; # ... with 21 more rows 11.2.2 Text-Daten einlesen Nun lesen wir Text-Daten ein; das können beliebige Daten sein. Eine gewisse Reichhaltigkeit ist von Vorteil. Nehmen wir das Parteiprogramm der Partei AfD3. library(pdftools) library(downloader) afd_url &lt;- &quot;https://www.alternativefuer.de/wp-content/uploads/sites/7/2016/05/2016-06-27_afd-grundsatzprogramm_web-version.pdf&quot; afd_pfad &lt;- &quot;data/afd_programm.pdf&quot; download(afd_url, afd_pfad) afd_raw &lt;- pdf_text(afd_pfad) afd_raw[3] #&gt; [1] &quot;3\\t Programm für Deutschland | Inhalt\\n 7 | Kultur, Sprache und Identität\\t\\t\\t\\t 45 9 | Einwanderung, Integration und Asyl\\t\\t\\t 57\\n 7.1 \\t\\t Deutsche Kultur, Sprache und Identität erhalten\\t 47 9.1\\t Keine irreguläre Einwanderung über das Asylrecht\\t 59\\n 7.2 \\t\\t Deutsche Leitkultur statt Multikulturalismus\\t\\t 47 9.1.1\\t Asylzuwanderung - für einen Paradigmenwechsel\\t 59\\n 7.3 \\t\\t Die deutsche Sprache als Zentrum unserer Identität\\t 47 9.1.2\\t Rückführung - Schluss mit Fehlanreizen und \\t\\t\\t\\n 7.4 \\t \\t Kultur und Kunst von Einflussnahme der Parteien befreien\\t 48 \\t\\t falscher Nachsicht\\t\\t\\t\\t\\t 60\\n 7.5 \\t\\t Für eine zeitgemäße Medienpolitik: Rundfunkbeitrag abschaffen\\t 48 9.2\\t Einwanderung aus EU-Staaten\\t\\t\\t\\t 61\\n 7.6 \\t\\t Der Islam im Spannungsverhältnis zu unserer Werteordnung\\t 48 9.3\\t Gesteuerte Einwanderung aus Drittstaaten\\t\\t 62\\n 7.6.1\\t\\t Der Islam gehört nicht zu Deutschland\\t\\t\\t 49 9.4\\t Integration - Mehr als nur Deutsch lernen\\t\\t 63\\n 7.6.2\\t\\t Kritik am Islam muss erlaubt sein\\t\\t\\t 49 9.5\\t Kosten der Einwanderung - Transparenz herstellen\\t 63\\n 7.6.3\\t\\t Auslandsfinanzierung von Moscheen beenden\\t \\t 49 9.6\\t Einwandererkriminalität - nichts verschleiern,\\n 7.6.4\\t\\t Keine öffentlich-rechtliche Körperschaft für \\t\\t nichts verschweigen\\t\\t\\t\\t\\t 64\\n \\t\\t\\t islamische Organisationen\\t\\t\\t\\t 50 9.7\\t Einbürgerung - Abschluss gelungener Integration\\t 65\\n 7.6.5\\t\\t Keine Vollverschleierung im öffentlichen Raum\\t 50\\n 10 | Wirtschaft, digitale Welt und Verbraucherschutz\\t 66\\n 8 | Schule, Hochschule und Forschung\\t\\t\\t 51 10.1\\t\\t Freier Wettbewerb sichert unseren Wohlstand\\t\\t 67\\n 8.1 \\t\\t Forschung und Lehre: In Freiheit und als Einheit\\t 52 10.2 \\t\\t Soziale Marktwirtschaft statt Planwirtschaft\\t\\t 67\\n 8.1.1\\t \\t Autonomie durch Grundfinanzierung stärken\\t \\t 52 10.3 \\t\\t Internationale Wirtschaftspolitik neu ausrichten\\t 67\\n 8.1.2\\t\\t Förderung der “Gender-Forschung” beenden\\t\\t 52 10.4 \\t\\t Hohe Standards für Handelsabkommen\\t\\t 68\\n 8.1.3\\t\\t Diplom, Magister und Staatsexamen wieder einführen\\t 52 10.5 \\t\\t Bürokratie abbauen\\t\\t\\t\\t\\t 68\\n 8.1.4\\t\\t Studienanforderungen erhöhen\\t\\t\\t 53 10.6 \\t\\t Den Technologiestandort Deutschland voranbringen\\t 68\\n 8.2 \\t\\t Unser Schulsystem: Stark durch Differenzierung\\t 53 10.7 \\t\\t Staatliche Subventionen reduzieren und befristen\\t 69\\n 8.2.1\\t\\t Die Einheitsschule führt zu Qualitätsverlust\\t\\t 53 10.8 \\t\\t Keine Privatisierung gegen den Willen der Bürger\\t 69\\n 8.2.2\\t\\t Wissensvermittlung muss zentrales Anliegen bleiben\\t 53 10.9 \\t\\t Der Mittelstand als Herz unserer Wirtschaftskraft\\t 69\\n 8.2.3\\t\\t Leistungsbereitschaft und Disziplin stärken\\t\\t 54 10.10 \\tDigitalisierung als Chance und Herausforderung\\t 69\\n 8.2.4\\t\\t Politisch-ideologische Indoktrination darf es an 10.10.1 Quelloffene Software und sichere Hardware\\t\\t 69\\n \\t\\t\\t der Schule nicht geben\\t\\t\\t\\t\\t 54 10.10.2 Sichere Kommunikation als Standortvorteil\\n 8.2.5\\t\\t Duale berufliche Bildung stärken und erhalten\\t \\t 54 \\t\\t und Bürgerrecht\\t\\t\\t\\t\\t 70\\n 8.2.6\\t\\t Keine Inklusion “um jeden Preis”. Förder- und 10.10.3 Deutsche Literatur im Inland digitalisieren\\t\\t 70\\n \\t\\t\\t Sonderschulen erhalten\\t\\t\\t\\t 54 10.11\\t\\t Verbraucherschutz modernisieren und stärken\\t\\t 70\\n 8.2.7 \\t\\t Koranschulen schließen. Islamkunde in den 10.11.1 Lebensmittel besser kennzeichnen\\t\\t\\t 71\\n \\t\\t\\t Ethikunterricht integrieren\\t\\t\\t\\t 55 10.11.2 Langlebige Produkte statt geplante Obsoleszenz\\t 71\\n 8.2.8 \\t Keine Sonderrechte für muslimische Schüler\\t\\t 55 10.11.3 Textilien und Kinderspielzeug auf Schadstoffe prüfen\\t 71\\n 8.3 \\t\\t Nein zu “Gender-Mainstreaming” und 10.11.4 Wasseraufbereitung modernisieren und verbessern\\t 71\\n \\t\\t\\t Frühsexualisierung\\t\\t\\t\\t\\t 55\\n 8.3.1 \\t\\t Keine “geschlechterneutrale” Umgestaltung der\\n \\t\\t\\t deutschen Sprache\\t\\t\\t\\t\\t 55\\n 8.3.2 \\t Geschlechterquoten sind leistungsfeindlich\\n \\t\\t\\t und ungerecht\\t\\t\\t\\t\\t\\t 56\\n&quot; Mit download haben wir die Datei mit der Url afd_url heruntergeladen und als afd_pfad gespeichert. Für uns ist pdf_text sehr praktisch, da diese Funktion Text aus einer beliebige PDF-Datei in einen Text-Vektor einliest. Der Vektor afd_raw hat 96 Elemente; zählen wir die Gesamtzahl an Wörtern. Dazu wandeln wir den Vektor in einen tidy text Dataframe um. Auch die Stopwörter entfernen wir wieder wie gehabt. afd_df &lt;- data_frame(Zeile = 1:96, afd_raw) afd_df %&gt;% unnest_tokens(token, afd_raw) %&gt;% filter(str_detect(token, &quot;[a-z]&quot;)) -&gt; afd_df count(afd_df) #&gt; # A tibble: 1 × 1 #&gt; n #&gt; &lt;int&gt; #&gt; 1 26396 Eine substanzielle Menge von Text. Was wohl die häufigsten Wörter sind? 11.2.3 Worthäufigkeiten auszählen afd_df %&gt;% na.omit() %&gt;% # fehlende Werte löschen count(token, sort = TRUE) #&gt; # A tibble: 7,087 × 2 #&gt; token n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 die 1151 #&gt; 2 und 1147 #&gt; 3 der 870 #&gt; # ... with 7,084 more rows Die häufigsten Wörter sind inhaltsleere Partikel, Präpositionen, Artikel… Solche sogenannten “Stopwörter” sollten wir besser herausfischen, um zu den inhaltlich tragenden Wörtern zu kommen. Praktischerweise gibt es frei verfügbare Listen von Stopwörtern, z.B. im Paket lsa. library(lsa) data(stopwords_de) stopwords_de &lt;- data_frame(word = stopwords_de) stopwords_de &lt;- stopwords_de %&gt;% rename(token = word) afd_df %&gt;% anti_join(stopwords_de) -&gt; afd_df Unser Datensatz hat jetzt viel weniger Zeilen; wir haben also durch anti_join Zeilen gelöscht (herausgefiltert). Das ist die Funktion von anti_join: Die Zeilen, die in beiden Dataframes vorkommen, werden herausgefiltert. Es verbleiben also nicht “Nicht-Stopwörter” in unserem Dataframe. Damit wird es schon interessanter, welche Wörter häufig sind. afd_df %&gt;% count(token, sort = TRUE) -&gt; afd_count afd_count %&gt;% top_n(10) %&gt;% knitr::kable() token n deutschland 190 afd 171 programm 80 wollen 67 bürger 57 euro 55 dafür 53 eu 53 deutsche 47 deutschen 47 Ganz interessant; aber es gibt mehrere Varianten des Themas “deutsch”. Es ist wohl sinnvoller, diese auf den gemeinsamen Wortstamm zurückzuführen und diesen nur einmal zu zählen. Dieses Verfahren nennt man “stemming” oder trunkieren. library(SnowballC) afd_df %&gt;% mutate(token_stem = wordStem(.$token, language = &quot;german&quot;)) %&gt;% count(token_stem, sort = TRUE) -&gt; afd_count afd_count %&gt;% top_n(10) %&gt;% knitr::kable() token_stem n deutschland 219 afd 171 deutsch 119 polit 88 staat 85 programm 81 europa 80 woll 67 burg 66 soll 63 Das ist schon informativer. Dem Befehl wordStem füttert man einen Vektor an Wörtern ein und gibt die Sprache an (Default ist Englisch4). Das ist schon alles. 11.2.4 Visualisierung Zum Abschluss noch eine Visualisierung mit einer “Wordcloud” dazu. library(wordcloud) wordcloud(words = afd_count$token_stem, freq = afd_count$n, max.words = 100, scale = c(2,.5), colors=brewer.pal(6, &quot;Dark2&quot;)) Man kann die Anzahl der Wörter, Farben und einige weitere Formatierungen der Wortwolke beeinflussen5. Weniger verspielt ist eine schlichte visualisierte Häufigkeitsauszählung dieser Art. afd_count %&gt;% top_n(30) %&gt;% ggplot() + aes(x = reorder(token_stem, n), y = n) + geom_col() + coord_flip() -&gt; p1 afd_df %&gt;% count(token, sort = TRUE) %&gt;% top_n(30) %&gt;% ggplot() + aes(x = reorder(token, n), y = n) + geom_col() + coord_flip() -&gt; p2 library(gridExtra) grid.arrange(p1, p2, ncol = 2) Die beiden Diagramme vergleichen die trunkierten Wörter mit den nicht trunktierten Wörtern. Mit reorder ordnen wir die Spalte token nach der Spalte n. 11.3 Sentiment-Analyse Eine weitere interessante Analyse ist, die “Stimmung” oder “Emotionen” (Sentiments) eines Textes auszulesen. Die Anführungszeichen deuten an, dass hier ein Maß an Verständnis suggeriert wird, welches nicht (unbedingt) von der Analyse eingehalten wird. Jedenfalls ist das Prinzip der Sentiment-Analyse im einfachsten Fall so: Schau dir jeden Token aus dem Text an. Prüfe, ob sich das Wort im Lexikon der Sentiments wiederfindet. Wenn ja, dann addiere den Sentimentswert dieses Tokens zum bestehenden Sentiments-Wert. Wenn nein, dann gehe weiter zum nächsten Wort. Liefere zum Schluss die Summenwerte pro Sentiment zurück. Es gibt Sentiment-Lexika, die lediglich einen Punkt für “positive Konnotation” bzw. “negative Konnotation” geben; andere Lexiko weisen differenzierte Gefühlskonnotationen auf. Wir nutzen hier dieses Lexikon (Remus, Quasthoff, and Heyer 2010). Der Einfachheit halber gehen wir im Folgenden davon aus, dass das Lexikon schon aufbereitet vorliegt. Die Aufbereitung unten im Abschnitt zur Vertiefung nachgelesen werden. 11.4 Vertiefung 11.4.1 Erstellung des Sentiment-Lexikons Der Zweck dieses Abschnitts ist es, eine Sentiment-Lexikon in deutscher Sprache einzulesen. Dazu wird das Sentiment-Lexikon dieser Quelle verwendet (CC-BY-NC-SA 3.0). In diesem Paper finden sich Hintergründe. Von dort lassen sich die Daten herunter laden. Im folgenden gehe ich davon aus, dass die Daten herunter geladen sind und sich im Working Directory befinden. Wir benötigen diese Pakete (es ginge auch über base): library(stringr) library(readr) library(dplyr) Dann lesen wir die Daten ein, zuerst die Datei mit den negativen Konnotationen: neg_df &lt;- read_tsv(&quot;SentiWS_v1.8c_Negative.txt&quot;, col_names = FALSE) names(neg_df) &lt;- c(&quot;Wort_POS&quot;, &quot;Wert&quot;, &quot;Inflektionen&quot;) glimpse(neg_df) Dann parsen wir aus der ersten Spalte (Wort_POS) zum einen den entsprechenden Begriff (z.B. “Abbau”) und zum anderen die Wortarten-Tags (eine Erläuterung zu den Wortarten-Tags findet sich hier). neg_df %&gt;% mutate(Wort = str_sub(Wort_POS, 1, regexpr(&quot;\\\\|&quot;, .$Wort_POS)-1), POS = str_sub(Wort_POS, start = regexpr(&quot;\\\\|&quot;, .$Wort_POS)+1)) -&gt; neg_df str_sub parst zuerst das Wort. Dazu nehmen wir den Wort-Vektor Wort_POS, und für jedes Element wird der Text von Position 1 bis vor dem Zeichen | geparst; da der Querstrich ein Steuerzeichen in Regex muss er escaped werden. Für POS passiert das gleiche von Position |+1 bis zum Ende des Text-Elements. Das gleiche wiederholen wir für positiv konnotierte Wörter. pos_df &lt;- read_tsv(&quot;SentiWS_v1.8c_Positive.txt&quot;, col_names = FALSE) names(pos_df) &lt;- c(&quot;Wort_POS&quot;, &quot;Wert&quot;, &quot;Inflektionen&quot;) pos_df %&gt;% mutate(Wort = str_sub(Wort_POS, 1, regexpr(&quot;\\\\|&quot;, .$Wort_POS)-1), POS = str_sub(Wort_POS, start = regexpr(&quot;\\\\|&quot;, .$Wort_POS)+1)) -&gt; pos_df Schließlich schweißen wir beide Dataframes in einen: bind_rows(&quot;neg&quot; = neg_df, &quot;pos&quot; = pos_df, .id = &quot;neg_pos&quot;) -&gt; sentiment_df sentiment_df %&gt;% select(neg_pos, Wort, Wert, Inflektionen, -Wort_POS) -&gt; sentiment_df knitr::kable(head(sentiment_df)) neg_pos Wort Wert Inflektionen neg Abbau -0.058 Abbaus,Abbaues,Abbauen,Abbaue neg Abbruch -0.005 Abbruches,Abbrüche,Abbruchs,Abbrüchen neg Abdankung -0.005 Abdankungen neg Abdämpfung -0.005 Abdämpfungen neg Abfall -0.005 Abfalles,Abfälle,Abfalls,Abfällen neg Abfuhr -0.337 Abfuhren Literaturverzeichnis "]
]
